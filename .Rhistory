error = function(e){e}
)
con
close(con)
croplands_urls = function(region, verbose = TRUE){
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
h = new_handle(dirlistonly=TRUE)
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
if(all(class(con) != 'error')){
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
} else {
reg = strsplit(region, '/')[[1]][1]
if(verbose) cat(paste0(reg, ': ', con), sep = '\n')
}
closeAllConnections()
}
lapply(regions, croplands_urls)
closeAllConnections()
base_url
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
con
class(con)
all(class(con) != 'error')
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
verbose = TRUE
con
var_lines
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
croplands_urls = function(region, verbose = TRUE){
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
h = new_handle(dirlistonly=TRUE)
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
if(all(class(con) != 'error')){
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
} else {
reg = strsplit(region, '/')[[1]][1]
if(verbose) cat(paste0(reg, ': ', con), sep = '\n')
}
closeAllConnections()
}
lapply(regions, croplands_urls)
lapply(regions, print)
base_url
https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
' https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/'
base_url
base_url==  'https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/'
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
base_url
croplands_urls(regions[4], TRUE)
croplands_urls = function(region, verbose = TRUE){
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
h = new_handle(dirlistonly=TRUE)
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
if(all(class(con) != 'error')){
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
} else {
reg = strsplit(region, '/')[[1]][1]
if(verbose) cat(paste0(reg, ': ', con), sep = '\n')
}
closeAllConnections()
}
croplands_urls(regions[4], TRUE)
closeAllConnections()
region
croplands_urls = function(region, verbose = TRUE){
print(region)
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
h = new_handle(dirlistonly=TRUE)
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
if(all(class(con) != 'error')){
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
} else {
reg = strsplit(region, '/')[[1]][1]
if(verbose) cat(paste0(reg, ': ', con), sep = '\n')
}
closeAllConnections()
}
croplands_urls(regions[4], TRUE)
croplands_urls(regions[[4]], TRUE)
croplands_urls = function(region, verbose = TRUE){
print(region)
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
h = new_handle(dirlistonly=TRUE)
print(base_url)
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
if(all(class(con) != 'error')){
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
} else {
reg = strsplit(region, '/')[[1]][1]
if(verbose) cat(paste0(reg, ': ', con), sep = '\n')
}
closeAllConnections()
}
croplands_urls(regions[[4]], TRUE)
'https://e4ftl01.cr.usgs.gov/MEASURES/GSFAD30AFCE.001/2013.01.01/'==
'https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/'
intersect('https://e4ftl01.cr.usgs.gov/MEASURES/GSFAD30AFCE.001/2013.01.01/',
'https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/')
intersect('https://e4ftl01.cr.usgs.gov/MEASURES/GSFAD30AFCE.001/2013.01.01/',
'https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/')
'https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/' ==
'https://e4ftl01.cr.usgs.gov/MEASURES/GFSAD30AFCE.001/2013.01.01/'
regions = list('GFSAD30SEACE.001/2013.01.01/',
'GFSAD30NACE.001/2008.01.01/',
'GFSAD30EUCEARUMECE.001/2013.01.01/',
'GFSAD30AFCE.001/2013.01.01/',
'GFSAD30SACE.001/2013.01.01/',
'GSFAD30SAAFGIRCE.001/2013.01.01/',
'GFSAD30SAAFGIRCE.001/2013.01.01/')
croplands_urls = function(region, verbose = TRUE){
#print(region)
#region = 'GFSAD30AFCE.001/2013.01.01/'
base_url = paste0('https://e4ftl01.cr.usgs.gov/MEASURES/', region)
h = new_handle(dirlistonly=TRUE)
#print(base_url)
con = tryCatch({
curl(base_url, 'r', h)
},
error = function(e){e}
)
if(all(class(con) != 'error')){
var_lines = readLines(con)
close(con)
href = var_lines[grep('href', var_lines)]
href_tif = href[grep('.tif', href)]
urls = lapply(strsplit(href_tif, "href"), '[', 2)
urls = lapply(strsplit(unlist(urls), ">"), '[', 1)
urls = unlist(lapply(urls, function(x) gsub('\\"', '', x)))
urls = urls[grep('.tif$', urls)]
urls = gsub('=', '', urls)
urls = paste0(base_url, urls)
dst = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
reg = strsplit(region, '/')[[1]][1]
fname = paste0(dst, '/', reg, '.csv')
write.csv(urls, fname, row.names = FALSE)
if(verbose) {
if(file.exists(fname)) {
cat(paste(reg, 'successfully written'), sep = '\n')
}
}
} else {
reg = strsplit(region, '/')[[1]][1]
if(verbose) cat(paste0(reg, ': ', con), sep = '\n')
}
closeAllConnections()
}
lapply(regions, croplands_urls)
regions = list('GFSAD30SEACE.001/2013.01.01/',
'GFSAD30NACE.001/2008.01.01/',
'GFSAD30EUCEARUMECE.001/2013.01.01/',
'GFSAD30AFCE.001/2013.01.01/',
'GFSAD30SACE.001/2013.01.01/',
'GFSAD30SAAFGIRCE.001/2013.01.01/')
lapply(regions, croplands_urls)
urls
## ping the files to pearcey
dst = "//pearceyhome.csiro.au/home_intel/war42q/LU"
dir.create(paste0(dst, '/urls'))
src = paste0("//lw-osm-22-cdc.it.csiro.au/OSM_CDC_LW_MMT_DATA_work/",
'/LU/USGS_NASA_CROPLANDS/urls')
list.files(src)
list.files(src, full.names = T)
lapply(f, function(x) file.copy(x, dst))
f = list.files(src, full.names = T)
f
lapply(f, function(x) file.copy(x, dst))
dst
## ping the files to pearcey
dst = "//pearceyhome.csiro.au/home_intel/war42q/LU/urls"
lapply(f, function(x) file.copy(x, dst))
350/24
x1 = 1:10
x2 = 11:20
x3 = 2:5
cbind(x1, x2, x3)
library(devtools)
library(Rtools)
library(RTools)
?install_bitbucket
r = 'https://bitbucket.csiro.au/projects/MMT/repos/gdm_workflow/browse/gdmEngine'
install_bitbucket(r)
install_bitbucket('MMT/gdm_workflow/gdmEngine')
install_bitbucket
bitbucket_remote
bitbucket_remote
parse_git_repo <- function(repo) {
if (grepl("^https://github|^git@github", repo)) {
params <- parse_github_url(repo)
} else {
params <- parse_repo_spec(repo)
}
params <- params[viapply(params, nchar) > 0]
if (!is.null(params$pull)) {
params$ref <- github_pull(params$pull)
params$pull <- NULL
}
if (!is.null(params$release)) {
params$ref <- github_release()
params$release <- NULL
}
params
}
tst = 'https://bitbucket.org/labprentice'
parse_git_repo <- function(repo) {
if (grepl("^https://github|^git@github", repo)) {
params <- parse_github_url(repo)
} else {
params <- parse_repo_spec(repo)
}
params <- params[viapply(params, nchar) > 0]
if (!is.null(params$pull)) {
params$ref <- github_pull(params$pull)
params$pull <- NULL
}
if (!is.null(params$release)) {
params$ref <- github_release()
params$release <- NULL
}
params
}
parse_git_repo(tst)
parse_repo_spec <- function(repo) {
username_rx <- "(?:(?<username>[^/]+)/)?"
repo_rx     <- "(?<repo>[^/@#]+)"
subdir_rx   <- "(?:/(?<subdir>[^@#]*[^@#/])/?)?"
ref_rx      <- "(?:@(?<ref>[^*].*))"
pull_rx     <- "(?:#(?<pull>[0-9]+))"
release_rx  <- "(?:@(?<release>[*]release))"
ref_or_pull_or_release_rx <- sprintf(
"(?:%s|%s|%s)?", ref_rx, pull_rx, release_rx
)
spec_rx  <- sprintf(
"^%s%s%s%s$", username_rx, repo_rx, subdir_rx, ref_or_pull_or_release_rx
)
params <- as.list(re_match(text = repo, pattern = spec_rx))
if (is.na(params$.match)) {
stop(sprintf("Invalid git repo specification: '%s'", repo))
}
params[grepl("^[^\\.]", names(params))]
}
parse_git_repo(tst)
install.packages('rematch')
library(rematch)
parse_git_repo <- function(repo) {
if (grepl("^https://github|^git@github", repo)) {
params <- parse_github_url(repo)
} else {
params <- parse_repo_spec(repo)
}
params <- params[viapply(params, nchar) > 0]
if (!is.null(params$pull)) {
params$ref <- github_pull(params$pull)
params$pull <- NULL
}
if (!is.null(params$release)) {
params$ref <- github_release()
params$release <- NULL
}
params
}
## required
library(rematch)
parse_repo_spec <- function(repo) {
username_rx <- "(?:(?<username>[^/]+)/)?"
repo_rx     <- "(?<repo>[^/@#]+)"
subdir_rx   <- "(?:/(?<subdir>[^@#]*[^@#/])/?)?"
ref_rx      <- "(?:@(?<ref>[^*].*))"
pull_rx     <- "(?:#(?<pull>[0-9]+))"
release_rx  <- "(?:@(?<release>[*]release))"
ref_or_pull_or_release_rx <- sprintf(
"(?:%s|%s|%s)?", ref_rx, pull_rx, release_rx
)
spec_rx  <- sprintf(
"^%s%s%s%s$", username_rx, repo_rx, subdir_rx, ref_or_pull_or_release_rx
)
params <- as.list(re_match(text = repo, pattern = spec_rx))
if (is.na(params$.match)) {
stop(sprintf("Invalid git repo specification: '%s'", repo))
}
params[grepl("^[^\\.]", names(params))]
}
parse_git_repo(tst)
repo = tst
grepl("^https://github|^git@github", repo)
username_rx <- "(?:(?<username>[^/]+)/)?"
username_rx
?parse_git_repo
install.packages('remotes')
library(remotes)
parse_repo_spec
parse_repo_spec = NULL
library(remotes)
parse_repo_spec
library(remotes)
parse_repo_spec
parse_git_repo(tst)
viapply
install_remotes
library(devtools)
remote
install_remotes
?install_remotes
install_remotes = function(pkg) {
pkg_prefix = sapply(strsplit(pkg, '::'), '[', 1)
pkg_github    = union(which(!grepl('::', pkg)), which(pkg_prefix == 'github'))
pkg_git       = which(pkg_prefix == 'git')
pkg_bitbucket = which(pkg_prefix == 'bitbucket')
pkg_bioc      = which(pkg_prefix == 'bioc')
pkg_svn       = which(pkg_prefix == 'svn')
pkg_url       = which(pkg_prefix == 'url')
pkg_local     = which(pkg_prefix == 'local')
if (length(pkg_github) != 0L)
devtools::install_github(
gsub('^github::', '', pkg[pkg_github]))
if (length(pkg_git) != 0L)
devtools::install_git(
sapply(strsplit(pkg[pkg_git], '::'), '[', 2))
if (length(pkg_bitbucket) != 0L)
devtools::install_bitbucket(
sapply(strsplit(pkg[pkg_bitbucket], '::'), '[', 2))
if (length(pkg_bioc) != 0L)
devtools::install_bioc(
sapply(strsplit(pkg[pkg_bioc], '::'), '[', 2))
if (length(pkg_svn) != 0L)
devtools::install_svn(
sapply(strsplit(pkg[pkg_svn], '::'), '[', 2))
if (length(pkg_url) != 0L)
devtools::install_url(
sapply(strsplit(pkg[pkg_url], '::'), '[', 2))
if (length(pkg_local) != 0L)
devtools::install_local(
sapply(strsplit(pkg[pkg_local], '::'), '[', 2))
}
pkg_prefix = sapply(strsplit(tst, '::'), '[', 1)
pkg_prefix
install_github('cwarecsiro/github/github')
install_github('cwarecsiro/github/github')
?install_github
library(devtools)
install_github('cwarecsiro/gdmEngine/gdmEngine')
install_github('cwarecsiro/gdmEngine/gdmEngine')
library(devtools)
setwd('Z:/users/bitbucket/tomsstuff')
document()
